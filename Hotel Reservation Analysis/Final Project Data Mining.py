# -*- coding: utf-8 -*-
"""FP DATMIN_v002.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10y8TV5hSN4S0_XSJmb_CiH4gdQXglUd-

# Final Project Data Mining
#### Oleh :
###### 1. Nur Shofiatun (5003211042)
###### 2. Putri Ayu Desita (5003211051)
"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

"""### Preprocessing Data"""

data = pd.read_csv('Hotel Reservations.csv')

data

data.info()

# Mengubah Data kategori dummy menjadi string untuk mempermudah visualisasi
columns_to_change = ['repeated_guest', 'required_car_parking_space']
for column in columns_to_change:
    data[column] = data[column].replace({0: 'No', 1: 'Yes'})
print(data[columns_to_change])

print(data['repeated_guest'].dtype)
print(data['required_car_parking_space'].dtype)

# Memeriksa nilai logis pada data yang tipe nya int dan float
conditions = (data[['no_of_adults', 'no_of_children', 'no_of_weekend_nights', 'no_of_week_nights',
                    'no_of_previous_cancellations', 'no_of_previous_bookings_not_canceled', 'avg_price_per_room']] >= 0)
data = data[conditions.all(axis=1)]
print(data)

"""**Nilai int dan float pada data telah sesuai** <br>
*jumlah kolom dan baris sesuai dengan data asli*
"""

# Memeriksa missing value pada data
data.isnull().sum()

"""**Data tidak memiliki missing value sehingga tidak perlu dilakukan imputasi missing value**"""

# Memeriksa duplikat pada data
data.duplicated().sum()

"""**Tidak ada data duplikat dalam data hotel**"""

# Menggabungkan data tanggal, bulan, dan tahun dalam satu kolom
# Mengidentifikasi baris dengan nilai hari yang tidak valid
# Mengubah nama kolom sesuai dengan yang diharapkan oleh pd.to_datetime()
data.rename(columns={'arrival_date': 'day', 'arrival_month': 'month', 'arrival_year': 'year'}, inplace=True)

# Menggabungkan kolom tanggal, bulan, dan tahun menjadi satu kolom datetime
data['arrival_datetime'] = pd.to_datetime(data[['year', 'month', 'day']], errors='coerce')

# Menampilkan baris dengan nilai yang invalid (NaT)
invalid_data = data[data['arrival_datetime'].isna()]

print("Baris dengan nilai yang invalid:")
print(invalid_data)

"""**karena bulan 2 pada tahun 2018 hanya sampai tanggal 28, sehingga untuk data yang tanggal 29-2-2018 akan dibenarkan menjadi 28-2-2018**"""

# Mengoreksi nilai yang tidak valid (misalnya, mengatur hari menjadi 1 jika tidak valid)
data.loc[data['arrival_datetime'].isna(), 'arrival_date'] = 1

# Menggabungkan kembali kolom tanggal, bulan, dan tahun menjadi satu kolom datetime
data['arrival_datetime'] = pd.to_datetime(data[['year', 'month', 'day']], errors='coerce')

print("Data setelah mengoreksi nilai yang invalid:")
print(data)

data = data.drop(columns="arrival_date")
data.info()

# Memeriksa Outlier pada data
# Membuat boxplot untuk kolom 'avg_price_per_room'
plt.figure(figsize=(8, 6))
sns.boxplot(x='avg_price_per_room', data=data)
plt.title('Boxplot of avg_price_per_room')
plt.xlabel('avg_price_per_room')
plt.ylabel('Values')
plt.show()

# Mengatasi the Outlier Data
max_value = data['avg_price_per_room'].max()
max_index = data['avg_price_per_room'].idxmax()
data = data.drop(max_index)

# Memeriksa Outlier pada data
# Membuat boxplot untuk kolom 'avg_price_per_room'
plt.figure(figsize=(8, 6))
sns.boxplot(x='avg_price_per_room', data=data)
plt.title('Boxplot of avg_price_per_room')
plt.xlabel('avg_price_per_room')
plt.ylabel('Values')
plt.show()

# Mengidentifikasi Outlier menggunakan Interquartile Range (IQR)
# Menghitung kuartil atas dan kuartil bawah
Q1 = data['avg_price_per_room'].quantile(0.25)
Q3 = data['avg_price_per_room'].quantile(0.75)

# Menghitung IQR
IQR = Q3 - Q1

# Menentukan batas bawah dan batas atas untuk outlier
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Mengidentifikasi outlier
outliers = data[(data['avg_price_per_room'] < lower_bound) | (data['avg_price_per_room'] > upper_bound)]
print("Outliers:")
outliers

data.describe()

# Menyimpan data dalam bentuk csv untuk membuat dashboard di RShiny
import csv
output_name = 'hotel.csv'
data.to_csv(output_name, index=False)

print(f"Cleaned data save to '{output_name}'")

"""### Feature Selection and Extraction"""

# Spliting Data sesuai dengan jenisnya
data_numeric = data.select_dtypes(include=[np.number])
data_categorical = data.select_dtypes(exclude=[np.number])

data_numeric

data_categorical

"""#### Multikolinearitas"""

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Menghitung VIF untuk setiap variabel numerik
vif_data = pd.DataFrame()
vif_data["Feature"] = data_numeric.columns
vif_data["VIF"] = [variance_inflation_factor(data_numeric.values, i) for i in range(len(data_numeric.columns))]

# Tampilkan hasil VIF
print("Variance Inflation Factor (VIF):")
print(vif_data)

data.drop(['no_of_adults', 'year', 'avg_price_per_room', 'arrival_datetime'], axis=1, inplace=True)
data

data_numeric = data.select_dtypes(include=[np.number])

# Menghitung VIF untuk setiap variabel numerik
vif_data = pd.DataFrame()
vif_data["Feature"] = data_numeric.columns
vif_data["VIF"] = [variance_inflation_factor(data_numeric.values, i) for i in range(len(data_numeric.columns))]

# Tampilkan hasil VIF
print("Variance Inflation Factor (VIF):")
print(vif_data)

"""#### ANOVA"""

data_categorical = data.select_dtypes(exclude=[np.number])
data_categorical

from scipy.stats import f_oneway, pearsonr, chi2_contingency

# Variabel target untuk uji ANOVA
target_variable = 'booking_status'

# Inisialisasi variabel untuk menyimpan hasil uji ANOVA yang signifikan
significant_anova = {}
threshold = 0.05  # Tingkat signifikansi

# Melakukan uji ANOVA untuk setiap variabel numerik
for col in data_numeric.columns:
    # Membuat daftar nilai dari masing-masing kelompok dalam 'booking_status'
    groups = [data_numeric[data[target_variable] == group][col].dropna() for group in data[target_variable].unique()]

    # Melakukan uji ANOVA
    f_statistic, p_value = f_oneway(*groups)

    # Menyimpan hasil uji ANOVA yang signifikan
    if p_value < threshold:
        significant_anova[col] = {
            'F-Statistic': f_statistic,
            'P-value': p_value
        }

# Menampilkan variabel yang memiliki hubungan signifikan dengan 'booking_status'
print("Variabel yang memiliki hubungan signifikan dengan 'booking_status':")
for col, result in significant_anova.items():
    print(f"{col}:")
    print(f"  F-Statistic: {result['F-Statistic']}")
    print(f"  P-value: {result['P-value']}")
    print()

"""#### Uji Chi-Square"""

# Variabel target untuk uji Chi-Square
target_variable = 'booking_status'

# Inisialisasi variabel untuk menyimpan hasil uji Chi-Square yang signifikan
significant_chi2 = {}
threshold = 0.05  # Tingkat signifikansi

# Melakukan uji Chi-Square untuk setiap variabel kategorikal
for col in data_categorical.columns:
    if col != target_variable:
        contingency_table = pd.crosstab(data[target_variable], data[col])

        # Melakukan uji Chi-Square
        chi2, p_value, dof, expected = chi2_contingency(contingency_table)

        # Menyimpan hasil uji Chi-Square yang signifikan
        if p_value < threshold:
            significant_chi2[col] = {
                'Chi-Square Statistic': chi2,
                'P-value': p_value,
                'Degrees of Freedom': dof,
                'Expected Frequencies': expected
            }

# Menampilkan variabel yang memiliki hubungan signifikan dengan 'booking_status'
print("Variabel yang memiliki hubungan signifikan dengan 'booking_status':")
for col, result in significant_chi2.items():
    print(f"{col}:")
    print(f"  Chi-Square Statistic: {result['Chi-Square Statistic']}")
    print(f"  P-value: {result['P-value']}")
    print(f"  Degrees of Freedom: {result['Degrees of Freedom']}")
    print()

new_columns = ['booking_status', 'type_of_meal_plan', 'required_car_parking_space', 'room_type_reserved', 'market_segment_type', 'repeated_guest', 'no_of_children','no_of_weekend_nights', 'no_of_week_nights','lead_time', 'month', 'day', 'no_of_previous_cancellations', 'no_of_previous_bookings_not_canceled', 'no_of_special_requests']

df = data[new_columns]
df

"""### Oversampling"""

from sklearn.preprocessing import LabelEncoder

data_numeric = df.select_dtypes(include=[np.number])
data_categorical = df.select_dtypes(exclude=[np.number])

label_encoders = {}
for column in data_categorical.columns:
    le = LabelEncoder()
    data_categorical[column] = le.fit_transform(data_categorical[column])
    label_encoders[column] = le

encoded_df = pd.concat([data_numeric, data_categorical], axis=1)
encoded_df

"""#### Checking Oversampling"""

import matplotlib.pyplot as plt

# Memeriksa distribusi kelas dari variabel target
class_distribution = df[target_variable].value_counts()

# Menampilkan distribusi kelas
print(class_distribution)

# Visualisasi distribusi kelas dengan bar plot
sns.barplot(x=class_distribution.index, y=class_distribution.values)
plt.title('Distribusi Kelas dari Variabel Target')
plt.xlabel('Kelas')
plt.ylabel('Jumlah')
plt.show()

"""##### Metode SMOTE"""

from imblearn.over_sampling import SMOTE

# Memisahkan fitur dan variabel target
X = encoded_df.drop(columns=['booking_status'])
y = encoded_df['booking_status']

# Melakukan oversampling menggunakan SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Memeriksa kembali distribusi kelas setelah oversampling
class_distribution_resampled = pd.Series(y_resampled).value_counts()
print(class_distribution_resampled)

# Visualisasi distribusi kelas setelah oversampling
sns.barplot(x=class_distribution_resampled.index, y=class_distribution_resampled.values)
plt.title('Distribusi Kelas dari Variabel Target setelah Oversampling')
plt.xlabel('Kelas')
plt.ylabel('Jumlah')
plt.show()

"""### Classification and Training - Testing

#### Repeated holdout
"""

from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, recall_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

# Inisialisasi jumlah iterasi
n_iterations = 5

# Fungsi untuk menghitung spesifisitas
def specificity_score(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return tn / (tn + fp)

# Inisialisasi list untuk menyimpan hasil akurasi, spesifisitas, sensitivitas, dan AUC
accuracy_scores_knn, accuracy_scores_rf, accuracy_scores_svm, accuracy_scores_nn = [], [], [], []
specificity_scores_knn, specificity_scores_rf, specificity_scores_svm, specificity_scores_nn = [], [], [], []
sensitivity_scores_knn, sensitivity_scores_rf, sensitivity_scores_svm, sensitivity_scores_nn = [], [], [], []
auc_scores_knn, auc_scores_rf, auc_scores_svm, auc_scores_nn = [], [], [], []

# Inisialisasi model
models = {
    'KNN': KNeighborsClassifier(),
    'RandomForest': RandomForestClassifier(),
    'SVM': SVC(probability=True),  # Tambahkan probability=True untuk mendapatkan probabilitas
    'NeuralNetwork': MLPClassifier(max_iter=300)
}

# Repeated holdout
n_repeats = 5
test_size = 0.3
random_state = 42

plt.figure(figsize=(14, 7))

for i in range(n_repeats):
    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=test_size, random_state=random_state + i)

    for model_name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]  # Probabilitas untuk kelas positif

        accuracy = accuracy_score(y_test, y_pred)
        specificity = specificity_score(y_test, y_pred)
        sensitivity = recall_score(y_test, y_pred)
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)

        if model_name == 'KNN':
            accuracy_scores_knn.append(accuracy)
            specificity_scores_knn.append(specificity)
            sensitivity_scores_knn.append(sensitivity)
            auc_scores_knn.append(roc_auc)
        elif model_name == 'RandomForest':
            accuracy_scores_rf.append(accuracy)
            specificity_scores_rf.append(specificity)
            sensitivity_scores_rf.append(sensitivity)
            auc_scores_rf.append(roc_auc)
        elif model_name == 'SVM':
            accuracy_scores_svm.append(accuracy)
            specificity_scores_svm.append(specificity)
            sensitivity_scores_svm.append(sensitivity)
            auc_scores_svm.append(roc_auc)
        elif model_name == 'NeuralNetwork':
            accuracy_scores_nn.append(accuracy)
            specificity_scores_nn.append(specificity)
            sensitivity_scores_nn.append(sensitivity)
            auc_scores_nn.append(roc_auc)

        # Plot ROC curve untuk setiap model pada setiap iterasi
        plt.plot(fpr, tpr, label=f'{model_name} Iterasi {i+1} (AUC = {roc_auc:.2f})')

# Print hasil rata-rata
print("Rata-rata Akurasi:")
print(f"KNN: {np.mean(accuracy_scores_knn)}")
print(f"RandomForest: {np.mean(accuracy_scores_rf)}")
print(f"SVM: {np.mean(accuracy_scores_svm)}")
print(f"NeuralNetwork: {np.mean(accuracy_scores_nn)}")

print("\nRata-rata Spesifisitas:")
print(f"KNN: {np.mean(specificity_scores_knn)}")
print(f"RandomForest: {np.mean(specificity_scores_rf)}")
print(f"SVM: {np.mean(specificity_scores_svm)}")
print(f"NeuralNetwork: {np.mean(specificity_scores_nn)}")

print("\nRata-rata Sensitivitas:")
print(f"KNN: {np.mean(sensitivity_scores_knn)}")
print(f"RandomForest: {np.mean(sensitivity_scores_rf)}")
print(f"SVM: {np.mean(sensitivity_scores_svm)}")
print(f"NeuralNetwork: {np.mean(sensitivity_scores_nn)}")

print("\nRata-rata AUC:")
print(f"KNN: {np.mean(auc_scores_knn)}")
print(f"RandomForest: {np.mean(auc_scores_rf)}")
print(f"SVM: {np.mean(auc_scores_svm)}")
print(f"NeuralNetwork: {np.mean(auc_scores_nn)}")

"""#### K-Fold CV"""

# Inisialisasi K-Fold Cross-Validation
k = 5
kf = KFold(n_splits=k, shuffle=True, random_state=42)

# Lakukan K-Fold Cross-Validation
fold = 1
for train_index, test_index in kf.split(X):
    print(f'Fold {fold}')
    fold += 1

    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Oversampling menggunakan SMOTE pada data training
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    for model_name, model in models.items():
        model.fit(X_train_resampled, y_train_resampled)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]  # Probabilitas untuk kelas positif

        accuracy = accuracy_score(y_test, y_pred)
        specificity = specificity_score(y_test, y_pred)
        sensitivity = recall_score(y_test, y_pred)
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)

        if model_name == 'KNN':
            accuracy_scores_knn.append(accuracy)
            specificity_scores_knn.append(specificity)
            sensitivity_scores_knn.append(sensitivity)
            auc_scores_knn.append(roc_auc)
        elif model_name == 'RandomForest':
            accuracy_scores_rf.append(accuracy)
            specificity_scores_rf.append(specificity)
            sensitivity_scores_rf.append(sensitivity)
            auc_scores_rf.append(roc_auc)
        elif model_name == 'SVM':
            accuracy_scores_svm.append(accuracy)
            specificity_scores_svm.append(specificity)
            sensitivity_scores_svm.append(sensitivity)
            auc_scores_svm.append(roc_auc)
        elif model_name == 'NeuralNetwork':
            accuracy_scores_nn.append(accuracy)
            specificity_scores_nn.append(specificity)
            sensitivity_scores_nn.append(sensitivity)
            auc_scores_nn.append(roc_auc)

        # Plot ROC curve untuk lipatan saat ini
        plt.plot(fpr, tpr, label=f'{model_name} Fold {fold-1} (AUC = {roc_auc:.2f})')

# Print hasil rata-rata
print("Rata-rata Akurasi:")
print(f"KNN: {np.mean(accuracy_scores_knn)}")
print(f"RandomForest: {np.mean(accuracy_scores_rf)}")
print(f"SVM: {np.mean(accuracy_scores_svm)}")
print(f"NeuralNetwork: {np.mean(accuracy_scores_nn)}")

print("\nRata-rata Spesifisitas:")
print(f"KNN: {np.mean(specificity_scores_knn)}")
print(f"RandomForest: {np.mean(specificity_scores_rf)}")
print(f"SVM: {np.mean(specificity_scores_svm)}")
print(f"NeuralNetwork: {np.mean(specificity_scores_nn)}")

print("\nRata-rata Sensitivitas:")
print(f"KNN: {np.mean(sensitivity_scores_knn)}")
print(f"RandomForest: {np.mean(sensitivity_scores_rf)}")
print(f"SVM: {np.mean(sensitivity_scores_svm)}")
print(f"NeuralNetwork: {np.mean(sensitivity_scores_nn)}")

print("\nRata-rata AUC:")
print(f"KNN: {np.mean(auc_scores_knn)}")
print(f"RandomForest: {np.mean(auc_scores_rf)}")
print(f"SVM: {np.mean(auc_scores_svm)}")
print(f"NeuralNetwork: {np.mean(auc_scores_nn)}")